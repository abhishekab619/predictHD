{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import dask.dataframe as dd\n",
        "from dask.distributed import Client\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from google.colab import drive\n",
        "import gc\n",
        "\n",
        "client = Client()\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "input_path = '/content/drive/My Drive/BB_Normalized_Monthly_Final/combined_data_minus_183.parquet'\n",
        "\n",
        "data = dd.read_parquet(input_path)\n",
        "\n",
        "#identify numeric columns\n",
        "#target for discretization\n",
        "numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "#apply imputation within each partition\n",
        "def apply_imputation(df):\n",
        "    imputer = SimpleImputer(strategy='mean')\n",
        "    return pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
        "\n",
        "#Dask imputation to handle missing values\n",
        "data[numeric_cols] = data[numeric_cols].map_partitions(apply_imputation, meta=data[numeric_cols])\n",
        "\n",
        "data = data.compute()\n",
        "\n",
        "#discretization\n",
        "#10 bins\n",
        "discretizer = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform')\n",
        "data[numeric_cols] = discretizer.fit_transform(data[numeric_cols])\n",
        "\n",
        "#PyTorch tensor conversion for embedding\n",
        "data_tensor = torch.tensor(data[numeric_cols].values, dtype=torch.long)\n",
        "\n",
        "#embedding layer\n",
        "class DataEmbedder(nn.Module):\n",
        "    def __init__(self, num_features, embedding_dim):\n",
        "        super(DataEmbedder, self).__init__()\n",
        "        self.embeddings = nn.ModuleList([nn.Embedding(10, embedding_dim) for _ in range(num_features)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded_features = [self.embeddings[i](x[:, i]) for i in range(len(self.embeddings))]\n",
        "        return torch.cat(embedded_features, dim=1)\n",
        "\n",
        "#embedder model initialization\n",
        "num_features = len(numeric_cols)\n",
        "embedding_dim = 10\n",
        "embedder = DataEmbedder(num_features, embedding_dim)\n",
        "embedded_data = embedder(data_tensor)\n",
        "\n",
        "# Save the final vectorized data back to Google Drive in Parquet format\n",
        "output_path = '/content/drive/My Drive/BB_Normalized_Monthly_Final/vectorized_combined_data_minus_183.parquet'\n",
        "pd.DataFrame(data).to_parquet(output_path)\n",
        "\n",
        "print(\"Final data has been successfully saved to:\", output_path)\n",
        "\n",
        "#clean up resources to free memory\n",
        "client.close()\n",
        "gc.collect()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RT1iet_b7jke",
        "outputId": "52256822-55f8-4ec6-d4dc-7880c2cb1bb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
            "Perhaps you already have a cluster running?\n",
            "Hosting the HTTP server on port 44599 instead\n",
            "  warnings.warn(\n",
            "INFO:distributed.scheduler:State start\n",
            "INFO:distributed.scheduler:  Scheduler at:     tcp://127.0.0.1:42025\n",
            "INFO:distributed.scheduler:  dashboard at:  http://127.0.0.1:44599/status\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:41471'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:41839'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:34039'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:46201'\n",
            "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:38207', name: 1, status: init, memory: 0, processing: 0>\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:38207\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:50308\n",
            "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:41215', name: 0, status: init, memory: 0, processing: 0>\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:41215\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:50332\n",
            "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:34289', name: 2, status: init, memory: 0, processing: 0>\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:34289\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:50318\n",
            "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:38079', name: 3, status: init, memory: 0, processing: 0>\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:38079\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:50346\n",
            "INFO:distributed.scheduler:Receive client connection: Client-7f6b1130-1124-11ef-aade-0242ac1c000c\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:50356\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:distributed.core:Event loop was unresponsive in Nanny for 3.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
            "INFO:distributed.core:Event loop was unresponsive in Nanny for 3.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
            "INFO:distributed.core:Event loop was unresponsive in Scheduler for 3.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
            "INFO:distributed.core:Event loop was unresponsive in Nanny for 3.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
            "INFO:distributed.core:Event loop was unresponsive in Nanny for 3.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
            "INFO:distributed.core:Event loop was unresponsive in Scheduler for 3.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
            "INFO:distributed.core:Event loop was unresponsive in Nanny for 3.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
            "INFO:distributed.core:Event loop was unresponsive in Nanny for 3.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
            "INFO:distributed.core:Event loop was unresponsive in Nanny for 3.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
            "INFO:distributed.core:Event loop was unresponsive in Nanny for 3.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
            "INFO:distributed.core:Event loop was unresponsive in Nanny for 3.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
            "INFO:distributed.core:Event loop was unresponsive in Nanny for 3.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
            "INFO:distributed.core:Event loop was unresponsive in Scheduler for 3.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
            "INFO:distributed.core:Event loop was unresponsive in Nanny for 3.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
            "INFO:distributed.core:Event loop was unresponsive in Nanny for 3.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
            "INFO:distributed.core:Event loop was unresponsive in Nanny for 3.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
            "INFO:distributed.core:Event loop was unresponsive in Scheduler for 3.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
            "INFO:distributed.core:Event loop was unresponsive in Nanny for 3.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
            "INFO:distributed.core:Event loop was unresponsive in Nanny for 3.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
            "INFO:distributed.core:Event loop was unresponsive in Nanny for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
            "INFO:distributed.core:Event loop was unresponsive in Nanny for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n"
          ]
        }
      ]
    }
  ]
}