{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import dask.dataframe as dd\n",
        "from dask.distributed import Client\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "# Initialize the Dask client\n",
        "client = Client()\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Folder path on Google Drive\n",
        "folder_path = '/content/drive/My Drive/BB_Normalized_Monthly_Final/'\n",
        "\n",
        "# File names to process\n",
        "file_names = [\n",
        "    'combined_data_minus_183_part_A.parquet',\n",
        "    'combined_data_minus_183_part_B.parquet',\n",
        "    'combined_data_minus_183_part_C.parquet'\n",
        "]\n",
        "\n",
        "# Process each file sequentially\n",
        "for file_name in file_names:\n",
        "    # Load the data using Dask\n",
        "    data = dd.read_parquet(folder_path + file_name)\n",
        "\n",
        "    # Identify numerical columns\n",
        "    numerical_cols = data.select_dtypes(include=['number']).columns\n",
        "\n",
        "    # Apply standard scaling to numerical columns using a custom transformation function\n",
        "    def scale_columns(df):\n",
        "        scaler = StandardScaler()\n",
        "        df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
        "        return df\n",
        "\n",
        "    # Use map_partitions with the custom function\n",
        "    data = data.map_partitions(scale_columns)\n",
        "\n",
        "    # One-hot encoding is handled similarly if needed, using dask_ml.preprocessing or similar approach\n",
        "\n",
        "    # Compute the final DataFrame and save the results\n",
        "    vectorized_data = data.compute()\n",
        "    output_file_name = file_name.replace('.parquet', '_vectorized.parquet')\n",
        "    vectorized_data.to_parquet(folder_path + output_file_name)\n",
        "    print(f\"Vectorized data saved to {output_file_name}\")\n",
        "\n",
        "    # Clean up to free memory\n",
        "    del data, vectorized_data\n",
        "\n",
        "# Close the Dask client\n",
        "client.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MU_UpKFoH_bw",
        "outputId": "2d7745c9-9d3f-47d8-ec25-d4ef605d6555"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
            "Perhaps you already have a cluster running?\n",
            "Hosting the HTTP server on port 38597 instead\n",
            "  warnings.warn(\n",
            "INFO:distributed.scheduler:State start\n",
            "INFO:distributed.scheduler:  Scheduler at:     tcp://127.0.0.1:34293\n",
            "INFO:distributed.scheduler:  dashboard at:  http://127.0.0.1:38597/status\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:45933'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:43787'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:41487'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:44909'\n",
            "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:40619', name: 1, status: init, memory: 0, processing: 0>\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:40619\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:49476\n",
            "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:41921', name: 3, status: init, memory: 0, processing: 0>\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:41921\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:49492\n",
            "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:44193', name: 0, status: init, memory: 0, processing: 0>\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:44193\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:49490\n",
            "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:37063', name: 2, status: init, memory: 0, processing: 0>\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:37063\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:49508\n",
            "INFO:distributed.scheduler:Receive client connection: Client-39d3ff51-117a-11ef-84ac-0242ac1c000c\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:49516\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:distributed.worker.memory:Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 1.38 GiB -- Worker memory limit: 1.86 GiB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vectorized data saved to combined_data_minus_183_part_A_vectorized.parquet\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:distributed.worker.memory:Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 1.40 GiB -- Worker memory limit: 1.86 GiB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vectorized data saved to combined_data_minus_183_part_B_vectorized.parquet\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:distributed.worker.memory:Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 1.39 GiB -- Worker memory limit: 1.86 GiB\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:45933'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:43787'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:41487'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:44909'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:49490; closing.\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:49476; closing.\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:49508; closing.\n",
            "INFO:distributed.scheduler:Remove worker <WorkerState 'tcp://127.0.0.1:44193', name: 0, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1715640246.8026288')\n",
            "INFO:distributed.scheduler:Remove worker <WorkerState 'tcp://127.0.0.1:40619', name: 1, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1715640246.8051732')\n",
            "INFO:distributed.scheduler:Remove worker <WorkerState 'tcp://127.0.0.1:37063', name: 2, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1715640246.8081586')\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:49492; closing.\n",
            "INFO:distributed.scheduler:Remove worker <WorkerState 'tcp://127.0.0.1:41921', name: 3, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1715640246.8128731')\n",
            "INFO:distributed.scheduler:Lost all workers\n",
            "INFO:distributed.batched:Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:34293 remote=tcp://127.0.0.1:49492>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/distributed/batched.py\", line 115, in _background_send\n",
            "    nbytes = yield coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 767, in run\n",
            "    value = future.result()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/distributed/comm/tcp.py\", line 268, in write\n",
            "    raise CommClosedError()\n",
            "distributed.comm.core.CommClosedError\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vectorized data saved to combined_data_minus_183_part_C_vectorized.parquet\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:distributed.scheduler:Scheduler closing due to unknown reason...\n",
            "INFO:distributed.scheduler:Scheduler closing all comms\n"
          ]
        }
      ]
    }
  ]
}